---
title:  "Tidy Data: basic transformations"
type:   content
---

# Data

Today we'll continue working with the [College Majors](https://github.com/fivethirtyeight/data/tree/master/college-majors) data set from FiveThirtyEight.  If you need to reload it into your current R session use [this URL](https://github.com/fivethirtyeight/data/raw/master/college-majors/recent-grads.csv), or you've started the assignment for today you should have a local copy in your "data/" folder.

As we've noted, this data set is already tidy:  it has one row per observation and one column for each variable.  Today we'll see how easy it is to work with data that is properly formatted; next Tuesday we'll look at tools that make it easy to re-format not-so-tidy data sets.

<br>

#  Side bar:  file paths

First, a mea culpa.  There was a bug in the original version of the assignment file for today that some of you ran into.  

##

The original file had this line:

```{r eval=FALSE}
recent_grads <- readr::read_csv("~/bio185/data/recent-grads.csv")
```

When it should have had this line:

```{r message=FALSE}
recent_grads <- readr::read_csv("data/recent-grads.csv")
```

##

To explain, here's a very brief primer on Linux/Unix/MacOS file paths.  [Linux](https://en.wikipedia.org/wiki/Linux) (which is what you are using on the RNA server) and MacOS ([Darwin](https://en.wikipedia.org/wiki/Darwin_(operating_system))) share a lineage; they are both derived from [Unix](https://en.wikipedia.org/wiki/Unix), so all three operating system families have a lot in common, including how they refer to resources on a device.

This string represents the root of the system (container that holds everything):

```
"/"
```

##

On RNA, all of your files live in a subfolder in the `home` directory that is in turn in this root folder:

```
"/home/your-user-name"
```

##

This is a short cut to your home folder:

```
"~"
```

Finally, paths can be absolute or relative.  If they are absolute they start with that root folder "/", if they are relative they don't and are relative to your current working directory.  

##

To see your current working directory:

```{r}
getwd()
```

##

So hopefully you can now understand the bug in the original version of this assignment.  This incorrect line refereed to a directory that existed for me (I had a bio185 subfolder in my home directory with a data subfolder in it), but likely didn't exist for you!

```{r eval=FALSE}
recent_grads <- readr::read_csv("~/bio185/data/recent-grads.csv")
```

vs:

```{r eval=FALSE}
recent_grads <- readr::read_csv("data/recent-grads.csv")
```

<br>

# Calculating new values

On Tuesday we saw an example of how we could change the data type of an existing column when we switched the `Major_category` variable from being a `character` vector to a `factor` vector.  We can use exactly the same syntax to add new variables to a table:  instead of assigning values into an existing column (which overwrites it), you can assign a new vector into a column with a name that doesn't exist yet.

Many of you noted on Tuesday that it wasn't really possible to put together a visualization comparing the employment data between majors because the total number of people surveyed for each was variable.  So we'd like to calculate some percentage employment numbers.  Vector operations in R make this extremely easy to do.

##

Let's make a new column that holds the percentage of people who are employed for each major:

```{r}
recent_grads$percent_employed <- recent_grads$Employed / recent_grads$Total
```

Take a moment to make sure you understand what we did there.

##

Now make three more columns on your own for
* Percentage of people with jobs that require a college degree
* Percentage of people with jobs that don't
* Percentage of people in low wage jobs

##

We can use any mathematical expression we want on the right hand side of assignment operations.  For example if we want to calculate how different each major is from the mean of the employment percentage:

```{r}
recent_grads$employment_dev <- recent_grads$percent_employed - 
                               mean(recent_grads$percent_employed)
```

<br>

# Filtering

We've already seen some simple examples of how you can use indexing syntax to filter data held in variables.  The old-school way of filtering rows in tables is to setup complex indexing operations.  The new-school way, which is described in the reading for today, is to use `dplyr` which is part of the `tidyverse`.

```{r}
library(dplyr)
```

The `dplyr` package contains lots of useful tools for transforming data sets including `filter`.  On Tuesday all of the visualizations I showed only used the `Major_category` variable because there are too many individual majors to plot each on an axis.

##

Let's use `filter` to create a table holding data for just the Biology majors:

```{r}
bio <- filter(recent_grads, Major_category == "Biology & Life Science")
```

Take a look at the resulting table.

##

Now we have a reasonable number of majors to plot on a categorical axis:

```{r}
library(ggplot2)
```

```{r}
ggplot(bio, aes(Major, percent_employed)) + 
  geom_bar(stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

See `?geom_bar` for an explanation of the `stat` argument, and today's homework for an explanation of how to angle text on an axis.

<br>

# Re-arranging

That plot was ok, but a little messy looking because the x-axis was arranged alphabetically by major rather than by the percentage employment.  The problem is that our `Major` variable is still a character vector.  We'll need to turn it into an ordered factor.  We can easily do this with two other `dplyr` functions called `arrange` and `mutate`.

First, let's make a new version of the biology major table that is `arranged` (sorted) by `percent_employed`:

```{r}
bio_sorted <- arrange(bio, percent_employed)
```

Take a look at this new table.

Next, since we're doing things the `dplyr` way today, we'll use `mutate` to change the `Major` column into an ordered factor (you could have used assignment to do this like we did on Tuesday).

##

```{r}
bio_ordered <- mutate( bio_sorted, 
                       Major = factor(Major, levels = Major, ordered = TRUE)
                     )
```

Take a moment to dissect what we did there.  See `bio_ordered$Major`.

##

Which makes for a prettier plot:

```{r}
ggplot(bio_ordered, aes(Major, percent_employed)) + 
  geom_bar(stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

##

Check out the [Data Wrangling Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) for lots of great visual examples of how to use functions in `dplyr`.

# The Pipe

If we assemble everything we just did into one code block, we'd have:

```{r eval=FALSE}
bio         <- filter(recent_grads, Major_category == "Biology & Life Science")
bio_sorted  <- arrange(bio, percent_employed)
bio_ordered <- mutate( bio_sorted, 
                       Major = factor(Major, levels = Major, ordered = TRUE)
                     )
ggplot(bio_ordered, aes(Major, percent_employed)) + 
  geom_bar(stat = 'identity') + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

That's a little messy and takes a bit of effort to understand, partly because we are creating a bunch of intermediate variables that we're only using once.  To clean up code blocks like this, the `tidyverse` recommends using a new kind of operator called the pipe:

```
%>%
```

The pipe flows data (usually a table) from left to right. Compare this to the above:

```{r eval=FALSE}
recent_grads %>%
  filter(Major_category == "Biology & Life Science") %>%
  arrange(percent_employed) %>%
  mutate(Major = factor(Major, levels = Major, ordered = TRUE)) %>%
  ggplot(aes(Major, percent_employed)) + 
    geom_bar(stat = 'identity') + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Notice how we no longer have to pass the table in as the first argument to each function.  The pipe is doing that automatically for us.

For this class you're welcome to use either style (pipped, or with intermediate variables).

<br>

# Functions!

That block of code is great if we're only interested in Biology, but what if we now want to do the same for the engineering majors?  One option would be "copy-paste-edit".  However, that's a [terrible idea](https://en.wikipedia.org/wiki/Don't_repeat_yourself).  There are lots of complex reasons why (feel free to ask if you're interested in computer science); I'd recommend you trust me, it's a terrible idea.

Fortunately, we've got a great tool we can use when we want to take a piece of code and make it reusable with modifications:  functions!

So far you've been using functions that R, or an R package, have provided for you.  But you can also write your own.

##

A very basic function looks like this:

```{r}
func <- function() {
  
}
```

##

We can then call our little function like any other:

```{r}
func()
```

##

Sadly, it doesn't do much yet.  Let's add in some arguments:

```{r}
func <- function(a, b) {
  a + b
}
```

##

```{r}
func(1, 2)
```

##

In R the "value" of a function (the return) is the value of the last computation:

```{r}
func <- function(a, b) {
  a + b
  a - b
}
func(1, 2)
```

##

You can optionally provide a default value for arguments:

```{r}
func <- function(a, b = 10) {
  a + b
}
func(1, 2)
func(1)
```

##

Let's write a function that filters, arranges and plots majors for a category:

```{r}
plot_percent_employed <- function(category) {
  recent_grads %>%
  filter(Major_category == category) %>%
  arrange(percent_employed) %>%
  mutate(Major = factor(Major, levels = Major, ordered = TRUE)) %>%
  ggplot(aes(Major, percent_employed)) + 
    geom_bar(stat = 'identity') + 
    theme(axis.text.x = element_text(angle = 60, hjust = 1))
}
```

What have we don here?  Test it out!

##

Now use the tools we've learned today to continue exploring this data set!

<br>

# Side bar for Programmers: R scoping semantics

This side bar is here for folks coming to R from other programming languages like Java or Python.  It's meant to explain what might seem to you to be highly non-intuitive behavior; it might also blow your mind a little.  

You have been warned.

## Variable scope

In programming languages [scoping](https://en.wikipedia.org/wiki/Scope_(computer_science)) rules are the pattern that is used to identify the data you are referring to when you use a variable in an expression.  R implements a idiom called [lexical scope](https://en.wikipedia.org/wiki/Scope_(computer_science)#Lexical_scope_vs._dynamic_scope) and all functions in R are lexical [closures](https://en.wikipedia.org/wiki/Closure_(computer_programming)).

Here's what you need to know: functions can access data in the outer environment, but the outside environment can't reach inside a function.  Let's look at some examples.

##

The outside environment can't peak into functions:

```{r}
f1 <- function(a, b) {
  a + b
}

f1(10 , 20)
```

```{r}
a <- 11
a
f1(10, 20)
```

##

But functions can reach up to the outside:

```{r}
c <- 100

f2 <- function(a, b) {
  a + b + c
}

f2(10, 20)
```

```{r}
c <- 200
f2(10, 20)
```

Notice that the value of a variable at the time a function is created doesn't matter; we could change the value of `c` in the outer environment and that change was reflect in the `c` being referenced inside of our function.  It's not the value of outer variables that gets captured, it's the *variable binding* itself.  In this context functions are called "closures" because the enclose all variables that are within the lexical scope.

##

This behavior is recursive: values can be referenced all the way back up to the global environment.  For example:

```{r}
c  <- 100
f3 <- function(d) {
  function(a, b) {
    a + b + c + d
  }
}
```

In this case our `f3` is a function that returns a function (so [meta](https://en.wikipedia.org/wiki/Metaprogramming)).

```{r}
f3(10)
f4 <- f3(1)
f4(10, 20)
```

```{r}
f5 <- f3(2)
f5(10, 20)
```

This would be an equivalent of the last line (even if it is a bit [iffy](https://en.wikipedia.org/wiki/Immediately-invoked_function_expression)):

```{r}
f3(2)(10, 20)
```

## Pass-by-value and immutability

So, we've covered the first part of scoping rules in R: functions can access variables in the environment in which they were created.  Now we'll consider the second part: you can use values in other environments but you can't ever change them (well you *can*, but you *never, ever, should*; don't even think about looking up the `<<-` operator).  In computer science lingo, R uses [pass-by-value](https://en.wikipedia.org/wiki/Evaluation_strategy) semantics and (nearly) all data types are immutable.  The implementation is based on the standard [copy-on-write](https://en.wikipedia.org/wiki/Copy-on-write) optimization: data aren't actually replicated in memory until a new copy is needed for an operation that's going to mutate it.

##

Let's see it in action:

```{r}
c  <- 100
f6 <- function(a, b) {
  print(c)
  c <- 200
  print(c) 
  
  a + b + c
}
f6(10, 20)
c
```

The value of `c` was only changed inside of the function `f6`; this change didn't pollute the value of `c` in the outer environment.

##

Here's an example that might be surprising for folks who have worked in other dynamic languages:

```{r}
l  <- list(a = 10, b = 20)
f7 <- function(mutantList) {
  mutantList$c <- 30
  
  mutantList
}
f7(l)
l
```

The reason this choice was made is that R is designed as a data analysis environment.  Language tools that protect you from accidentally changing your data in hard to find ways are good thing!  If you absolutely must mutate and up-value there is the `<<-` operator.  But don't ever do this; it's a terrible idea.

## And now for something crazy: being lazy

Here's where things in R will start to seem really strange if you're coming to R from Java or Python and haven't seen a pure [functional programming language](https://en.wikipedia.org/wiki/Functional_programming) before.  

The scoping rules and functional semantics above combine with a third aspect of the R language to allow for some really interesting design idioms: [lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation).  In languages with lazy evaluation, expressions aren't actually evaluated *until their values are needed*.  Instead expressions initially become [promises](https://en.wikipedia.org/wiki/Futures_and_promises); they "promise" to have a value should it ever actually be used.  Trippy, right?

Let's look at an example.  Here's an idiom you'll frequently see in the R standard library to allow flexibility in defining default values for functions:

```{r}
area <- function(l, w = l) {
  l * w
}
area(2, 3)
area(2)
```

Python would not have been cool with that.

Again, owing to its functional roots, everything in R is an expression and **there are no statements**. Syntax like `if` and `for` blocks are *just expressions*.

So you can do things like this:

```{r}
area <- function( l
                , w = if (square) { l }
                , square = if (l == w) { TRUE } else { FALSE } 
                ) {
  
  if (square) { print("It's a square!") }
  
  l * w
}
area(2, 3)
area(2, 2)
area(2, square = TRUE)
```

This works because the expressions for `l`, `w` and `square` aren't actually evaluated until they are used in the body of the function.  Up to that point in the code they're just promises and promises evaluate in the run time scope of the closure. So, it's perfectly fine to define a set of expressions that have static cyclical dependencies as long as they will be evaluated in an appropriate order at run time.

This is either really awesome or totally insane, depending on your perspective.
